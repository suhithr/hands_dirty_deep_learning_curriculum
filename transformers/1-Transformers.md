Source: https://github.com/jacobhilton/deep_learning_curriculum/blob/master/1-Transformers.md

# Chapter 1: Transformers

The transformer is an important neural network architecture used for language modeling.

## Recommended reading

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Section 3 of the paper that introduced the transformer explains the architecture. Don't worry too much about the encoder and how that fits in, as that's somewhat specific to translation â€“ unsupervised transformer language models are generally decoder-only.

## Optional reading

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - A blog post explaining the architecture more carefully. Read this if you're finding the original paper hard to follow.
- [GPT-3](https://arxiv.org/abs/2005.14165) - A 175-billion parameter decoder-only transformer language model that exhibits impressive meta-learning capabilities.
- [The Transformer Family](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/) - An overview of many transformer variants, including Transformer-XL, Image Transformer, Sparse Transformer, Reformer and Universal Transformer.
- [T5](https://arxiv.org/abs/1910.10683) - A careful study of different architectural details and training objectives for transformers.
- [Mixture-of-Experts](https://arxiv.org/abs/1701.06538) - A form of parameter sparsity used by some more recent language models to improve training efficiency. Section 2 of this paper explains how they work.

## Suggested exercise

Implement a decoder-only transformer language model.

- Here are some first principle questions to answer:
    - What is different architecturally from the Transformer, vs a normal RNN, like an LSTM? (Specifically, how are recurrence and time managed?)
        - One key difference (and advantage to the transformer) is that an entire sequence of input tokens is processed at once and the output is produced one at a time. This is much faster than an RNN which needs to process one token at a time as the hidden state created by a token depends on the one before it.
        - The attention concept helps this as we can attend to the entire input at once and thus generate relationships between them. This also helps with longer-range dependencies that RNNs would have trouble managing. 

    - Attention is defined as, $Attention(Q,K,V) = softmax(QK^T/\sqrt(d_k))\cdot V$. What are the dimensions for Q, K, and V? Why do we use this setup? What other combinations could we do with (Q,K) that also output weights?
    
        - Given the number of elements in the query is $n$ then $Q: n \times d_k$ , $K: n\times d_k$ , $V: n\times d_v$ so thus the output is $n \times d_v$
        - In attention each query vector must attend to all the key vectors (dot product). These become weights for the value vectors for each query. Thus each query vector can be stacked into a matrix and the same for the keys & values. Matmuls are very fast on the GPU.

        > (Question): Thus the key & value matrices are then repetitions of the vectors? No. The key & value matrices are the packed key & value vectors generated by multiplying the input vectors with W_k and W_v.

        - The paper mentions an additive attention (vs. the proposed dot-product attention). Here we compute weights by passing the query through a feed-forward NN with 1 hidden layer. This involves concating $q$ and $k$ then a matmul in the hidden layer then a non-linear activation function and finally a matrix vector multiply with $v$. Dot product attention is just simpler and computationally more efficient.


        Some Useful Resources: https://tomekkorbak.com/2020/06/26/implementing-attention-in-pytorch/, https://lilianweng.github.io/posts/2018-06-24-attention/

    - Are the dense layers different at each multi-head attention block? Why or why not?

        -  Yes. Each head is projected with different learned projections so that the "head-local" attention can work on information in a different representation subspace. If we were in a single attention head then all the information that is learned by the head needs to get encoded into one matrix. The one head's output is the collection of a softmaxed Q.K times V. So say a word needs to attend to multiple words in the sentence, it has to get averaged out by the softmax as the sum is 1.
        Thus the multihead approach allows us to work around this representational limitation.
        
    - Why do we have so many skip connections, especially connecting the input of an attention function to the output? Intuitively, what if we didn't? 
        - We use the skip connections because the gradients of the layers after add+norm after which we insert a ReLU are zero half the time. So we could have vanishing gradients and thus parts of the network could become dead. The skip connections alleviate this.
        - They also help a bit with the outputs of the attention block which are weighted Values from V, this allows us to add some information from the input to the attention block into the output.

- Now we'll actually implement the code. Make sure each of these is completely correct - it's very easy to get the small details wrong.
    - Implement the positional embedding function first. 
    - Then implement the function which calculates attention, given (Q,K,V) as arguments. 
    - Now implement the masking function. 
    - Put it all together to form an entire attention block. 
    - Finish the whole architecture.
- If you get stuck, [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) may help, but don't just copy-paste the code.
- To check you have the attention mask set up correctly, train your model on a toy task, such as reversing a random sequence of tokens. The model should be able to predict the second half of the sequence, but not the first.
    - Finer architectural details either missing from the paper/that I missed while declaring I understand the transformer.
        - Generating outputs: The model generates a next token prediction for EACH token in the input sequence.
        - We are not using Label Smoothing for now for the loss function targets (unlike the paper).
- Finally, train your model on [the complete works of William Shakespeare](https://www.gutenberg.org/files/100/100-0.txt). Tokenize the corpus by splitting at word boundaries (`re.split(r"\b", ...)`). Make sure you don't use overlapping sequences as this can lead to overfitting.